{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "The softmax function is commonly used as the final layer in a neural network and in 'softmax' or multinomial logistic regression.\n",
    "\n",
    "Recall from your experience with logistic regression that the output predicts a binary class membership [0,1] and can be interpretecd as the probability of class 1 membership.\n",
    "\n",
    "Multinomal logistic regression is a generalization of logistic regression that allows the classifier to predict probability of membeship in k classes.\n",
    "\n",
    "Imagine a set of logistic regressions, predicting membership of each class in {0...k}\n",
    "\n",
    "\n",
    "$$ f(x) = \\begin{Bmatrix}\n",
    "            Pr(Y = 0) = sigmoid(\\theta_{0} * X) \\\\\n",
    "            Pr(Y = 1) = sigmoid(\\theta_{1} * X) \\\\\n",
    "            Pr(Y = 2) = sigmoid(\\theta_{2} * X) \\\\\n",
    "            Pr(Y = k) = sigmoid(\\theta_{k} * X)\n",
    "           \\end{Bmatrix}$$\n",
    "           \n",
    "An obvious problem in this method is that the output of each logistic regression is independent. This keeps us from using these outputs as an overall probability of class membership in class K.  Softmax squeezes the outputs of all these logistic regressions such that they sum to 1 and the outputs can be used as an overall class membership probability.\n",
    "\n",
    "$$ Pr(Y=k) =softmax \\begin{Bmatrix}\n",
    "            Pr(Y = 0) = sigmoid(\\theta_{0} * X) \\\\\n",
    "            Pr(Y = 1) = sigmoid(\\theta_{1} * X) \\\\\n",
    "            Pr(Y = 2) = sigmoid(\\theta_{2} * X) \\\\\n",
    "            Pr(Y = k) = sigmoid(\\theta_{k} * X)\n",
    "           \\end{Bmatrix}$$\n",
    "           \n",
    "           \n",
    "Mathematically, softmax looks like this:\n",
    "\n",
    "$$ \\theta{(z)}_j = \\frac{e^{z_{j}}}{\\sum^{K}_{k=1}{e^{z_k}}} $$ for j = 1...K\n",
    "\n",
    "\n",
    "\n",
    "Lets pretend we have the outputs from 5 logistic regressions that look like this:\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "z = np.array([0.9, 0.8, 0.2, 0.1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers obviously don't add up to 1.   I would interpret this as logit 0 and logit 1 strongly believe an observation is a member of these classes.   Logit 3 and 4 strongly believe against class membership.  Logit 5 is undecided.\n",
    "\n",
    "Lets apply softmax to these outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def softmax(z):\n",
    "    z_exp = [math.exp(x) for x in z]\n",
    "    sum_z_exp = sum(z_exp)\n",
    "    softmax = [round(i / sum_z_exp, 3) for i in z_exp]\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.284, 0.257, 0.141, 0.128, 0.19]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some neural networks, such as those used in vision tasks, can predict membership across thousands of classes. Softmax as a last layer allows us to know the probability some observation belongs to a particular class by squeezing the outputs into a single probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
